{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable Baselines3 튜토리얼 - Callbacks\n",
    "\n",
    "이 튜토리얼은 Stable Baselins3 라이브러리의 공식 튜토리얼과 예제 코드를 참고/번역하여 작성되었습니다.\n",
    "\n",
    "출처 : [https://github.com/Stable-Baselines-Team/rl-colab-notebooks](https://github.com/Stable-Baselines-Team/rl-colab-notebooks)\n",
    "\n",
    "Stable-Baselines3 Github: https://github.com/DLR-RM/stable-baselines3\n",
    "\n",
    "공식 문서: https://stable-baselines.readthedocs.io/en/master/\n",
    "\n",
    "공식 문서(Callbacks): https://stable-baselines3.readthedocs.io/en/master/guide/callbacks.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback 이란?\n",
    "\n",
    "일반적으로 callback 함수는 특정 이벤트나 조건이 발생했을 때 호출되는 함수입니다.\n",
    "\n",
    "Stable Baselines3에서 callback은 훈련 과정에서 호출하여 훈련 중에 RL 모델의 내부 상태에 접근할 수 있습니다.\n",
    "\n",
    "이를 통해 모니터링, 자동 저장, 모델 조작, 진행률 표시 등을 할 수 있습니다.\n",
    "\n",
    "wrapper와 마찬가지로 custom callback을 만들어서 사용할 수 있습니다.\n",
    "\n",
    "이 튜토리얼에서는 대표적인 callback 사용법을 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CheckpointCallback\n",
    "\n",
    "`CheckpointCallback`은 훈련 중에 모델을 저장하는 callback입니다.\n",
    "\n",
    "- 매 `save_freq` step 마다 모델을 저장합니다.\n",
    "- `save_replay_buffer`를 통해 replay buffer를 저장할 수 있습니다.\n",
    "- `save_vecnormalize`를 통해 normalized observation, reward를 저장할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x1340bb5d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "log_dir = \"./tmp/logs/checkpoint\"\n",
    "\n",
    "# Save a checkpoint every 1000 steps\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=1000,\n",
    "    save_path=log_dir,\n",
    "    name_prefix=\"rl_model\",\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "model = SAC(\"MlpPolicy\", \"Pendulum-v1\")\n",
    "model.learn(2000, callback=checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EvalCallback\n",
    "\n",
    "`EvalCallback`은 훈련 중에 모델을 평가하는 callback입니다.\n",
    "\n",
    "- 매 `eval_freq` step 마다 모델을 평가합니다.\n",
    "- `best_model_save_path`를 통해 최고 성능의 모델을 저장할 수 있습니다.\n",
    "- `log_path`를 통해 결과를 저장할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./tmp/logs/eval\n",
      "Eval num_timesteps=500, episode_reward=-1573.86 +/- 143.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 200       |\n",
      "|    mean_reward     | -1.57e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 500       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 15.5      |\n",
      "|    critic_loss     | 0.454     |\n",
      "|    ent_coef        | 0.888     |\n",
      "|    ent_coef_loss   | -0.19     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 399       |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.53e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 176       |\n",
      "|    time_elapsed    | 4         |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 24.6      |\n",
      "|    critic_loss     | 0.151     |\n",
      "|    ent_coef        | 0.813     |\n",
      "|    ent_coef_loss   | -0.33     |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=-1620.14 +/- 126.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 200       |\n",
      "|    mean_reward     | -1.62e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 31.1      |\n",
      "|    critic_loss     | 0.137     |\n",
      "|    ent_coef        | 0.767     |\n",
      "|    ent_coef_loss   | -0.419    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 899       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1500, episode_reward=-1548.35 +/- 152.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 200       |\n",
      "|    mean_reward     | -1.55e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1500      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 47.4      |\n",
      "|    critic_loss     | 0.115     |\n",
      "|    ent_coef        | 0.665     |\n",
      "|    ent_coef_loss   | -0.615    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1399      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.52e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 156       |\n",
      "|    time_elapsed    | 10        |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 50.3      |\n",
      "|    critic_loss     | 0.117     |\n",
      "|    ent_coef        | 0.647     |\n",
      "|    ent_coef_loss   | -0.624    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-1036.80 +/- 147.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 200       |\n",
      "|    mean_reward     | -1.04e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 2000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 62        |\n",
      "|    critic_loss     | 0.151     |\n",
      "|    ent_coef        | 0.583     |\n",
      "|    ent_coef_loss   | -0.577    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 1899      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -1.4e+03 |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 156      |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 2400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 71.1     |\n",
      "|    critic_loss     | 0.197    |\n",
      "|    ent_coef        | 0.532    |\n",
      "|    ent_coef_loss   | -0.565   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2299     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-1056.39 +/- 8.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 200       |\n",
      "|    mean_reward     | -1.06e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 2500      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 74.7      |\n",
      "|    critic_loss     | 0.179     |\n",
      "|    ent_coef        | 0.52      |\n",
      "|    ent_coef_loss   | -0.568    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 2399      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-873.70 +/- 163.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -874     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 80.9     |\n",
      "|    critic_loss     | 0.556    |\n",
      "|    ent_coef        | 0.472    |\n",
      "|    ent_coef_loss   | -0.59    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 2899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.27e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 146       |\n",
      "|    time_elapsed    | 21        |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 84.2      |\n",
      "|    critic_loss     | 0.467     |\n",
      "|    ent_coef        | 0.454     |\n",
      "|    ent_coef_loss   | -0.508    |\n",
      "|    learning_rate   | 0.0003    |\n",
      "|    n_updates       | 3099      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-283.84 +/- 198.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -284     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 86.1     |\n",
      "|    critic_loss     | 0.609    |\n",
      "|    ent_coef        | 0.43     |\n",
      "|    ent_coef_loss   | -0.344   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3399     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-150.92 +/- 48.90\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -151     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 89.5     |\n",
      "|    critic_loss     | 0.963    |\n",
      "|    ent_coef        | 0.394    |\n",
      "|    ent_coef_loss   | -0.356   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.09e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 140       |\n",
      "|    time_elapsed    | 28        |\n",
      "|    total_timesteps | 4000      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4500, episode_reward=-145.92 +/- 90.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -146     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 91.5     |\n",
      "|    critic_loss     | 1.07     |\n",
      "|    ent_coef        | 0.357    |\n",
      "|    ent_coef_loss   | -0.347   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4399     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -936     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 139      |\n",
      "|    time_elapsed    | 34       |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 91.3     |\n",
      "|    critic_loss     | 1.53     |\n",
      "|    ent_coef        | 0.335    |\n",
      "|    ent_coef_loss   | -0.461   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4699     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-121.05 +/- 72.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -121     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 87.7     |\n",
      "|    critic_loss     | 0.673    |\n",
      "|    ent_coef        | 0.319    |\n",
      "|    ent_coef_loss   | -0.657   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 4899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=5500, episode_reward=-187.54 +/- 115.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -188     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 87.2     |\n",
      "|    critic_loss     | 1.32     |\n",
      "|    ent_coef        | 0.282    |\n",
      "|    ent_coef_loss   | -0.576   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5399     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -820     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 5600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 93.3     |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    ent_coef        | 0.275    |\n",
      "|    ent_coef_loss   | -0.644   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5499     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-121.91 +/- 76.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -122     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 87.3     |\n",
      "|    critic_loss     | 1.59     |\n",
      "|    ent_coef        | 0.249    |\n",
      "|    ent_coef_loss   | -0.607   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -729     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 45       |\n",
      "|    total_timesteps | 6400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 89.1     |\n",
      "|    critic_loss     | 1.65     |\n",
      "|    ent_coef        | 0.228    |\n",
      "|    ent_coef_loss   | -0.436   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6299     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6500, episode_reward=-119.59 +/- 105.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 86.7     |\n",
      "|    critic_loss     | 1.26     |\n",
      "|    ent_coef        | 0.223    |\n",
      "|    ent_coef_loss   | -0.681   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6399     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=7000, episode_reward=-206.44 +/- 72.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -206     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 84.7     |\n",
      "|    critic_loss     | 1.93     |\n",
      "|    ent_coef        | 0.201    |\n",
      "|    ent_coef_loss   | -0.268   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 6899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -662     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 85.7     |\n",
      "|    critic_loss     | 1.5      |\n",
      "|    ent_coef        | 0.193    |\n",
      "|    ent_coef_loss   | -0.36    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7099     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7500, episode_reward=-73.67 +/- 58.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -73.7    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 84.6     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.182    |\n",
      "|    ent_coef_loss   | -0.456   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7399     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=-143.38 +/- 86.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -143     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 78       |\n",
      "|    critic_loss     | 2.55     |\n",
      "|    ent_coef        | 0.166    |\n",
      "|    ent_coef_loss   | -0.532   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -622     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 140      |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8500, episode_reward=-73.29 +/- 59.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -73.3    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 76.9     |\n",
      "|    critic_loss     | 1.86     |\n",
      "|    ent_coef        | 0.151    |\n",
      "|    ent_coef_loss   | -0.395   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8399     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -580     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 142      |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 8800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 79.1     |\n",
      "|    critic_loss     | 1.68     |\n",
      "|    ent_coef        | 0.143    |\n",
      "|    ent_coef_loss   | -0.345   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8699     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-168.18 +/- 95.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -168     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 84       |\n",
      "|    critic_loss     | 2.54     |\n",
      "|    ent_coef        | 0.138    |\n",
      "|    ent_coef_loss   | 0.103    |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 8899     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9500, episode_reward=-212.43 +/- 112.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -212     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9500     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 73       |\n",
      "|    critic_loss     | 1.66     |\n",
      "|    ent_coef        | 0.126    |\n",
      "|    ent_coef_loss   | -0.092   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9399     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -542     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 142      |\n",
      "|    time_elapsed    | 67       |\n",
      "|    total_timesteps | 9600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 60.3     |\n",
      "|    critic_loss     | 1.17     |\n",
      "|    ent_coef        | 0.124    |\n",
      "|    ent_coef_loss   | -0.819   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9499     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-119.68 +/- 75.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -120     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 70.4     |\n",
      "|    critic_loss     | 1.67     |\n",
      "|    ent_coef        | 0.114    |\n",
      "|    ent_coef_loss   | -0.484   |\n",
      "|    learning_rate   | 0.0003   |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x158612910>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "\n",
    "train_env = gym.make(\"Pendulum-v1\")\n",
    "# Separate evaluation env\n",
    "eval_env = gym.make(\"Pendulum-v1\")\n",
    "\n",
    "model = SAC(\"MlpPolicy\", train_env, verbose=1)\n",
    "\n",
    "log_dir = \"./tmp/logs/eval\"\n",
    "\n",
    "# $ tensorboard --logdir ./examples/tmp/logs/eval 로 tensorboard 실행\n",
    "new_logger = configure(log_dir, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "model.set_logger(new_logger)\n",
    "\n",
    "# Use deterministic actions for evaluation\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=500, n_eval_episodes=5,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "model.learn(total_timesteps=10000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CallbackList\n",
    "\n",
    "`CallbackList`는 여러 callback을 순차적으로 실행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=500, episode_reward=-1562.98 +/- 187.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1000, episode_reward=-1413.46 +/- 51.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1500, episode_reward=-1299.74 +/- 63.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2000, episode_reward=-1066.46 +/- 77.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=2500, episode_reward=-1237.29 +/- 39.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=3000, episode_reward=-292.39 +/- 136.51\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3500, episode_reward=-148.00 +/- 90.11\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=4000, episode_reward=-166.98 +/- 56.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=-269.00 +/- 93.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5000, episode_reward=-163.59 +/- 115.86\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x146a0e110>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import CallbackList, CheckpointCallback, EvalCallback\n",
    "\n",
    "log_dir = \"./tmp/logs/callback_list\"\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq=1000, save_path=log_dir)\n",
    "# Separate evaluation env\n",
    "eval_env = gym.make(\"Pendulum-v1\")\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path=log_dir,\n",
    "                             log_path=log_dir, eval_freq=500)\n",
    "# Create the callback list\n",
    "callback = CallbackList([checkpoint_callback, eval_callback])\n",
    "\n",
    "model = SAC(\"MlpPolicy\", \"Pendulum-v1\")\n",
    "# Equivalent to:\n",
    "# model.learn(5000, callback=[checkpoint_callback, eval_callback])\n",
    "model.learn(5000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. StopTrainingOnNoModelImprovement\n",
    "\n",
    "`StopTrainingOnNoModelImprovement`는 모델이 향상되지 않을 때 훈련을 중지하는 callback입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Creating environment from the given name 'Pendulum-v1'\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.57e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 217       |\n",
      "|    time_elapsed    | 3         |\n",
      "|    total_timesteps | 800       |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 27.2      |\n",
      "|    critic_loss     | 0.0582    |\n",
      "|    ent_coef        | 0.503     |\n",
      "|    ent_coef_loss   | -1.08     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 699       |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=-1726.12 +/- 90.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 200       |\n",
      "|    mean_reward     | -1.73e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 1000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 33.7      |\n",
      "|    critic_loss     | 0.0516    |\n",
      "|    ent_coef        | 0.417     |\n",
      "|    ent_coef_loss   | -1.25     |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 899       |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.51e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 213       |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 1600      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 51.9      |\n",
      "|    critic_loss     | 0.0734    |\n",
      "|    ent_coef        | 0.258     |\n",
      "|    ent_coef_loss   | -0.889    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1499      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-1009.15 +/- 262.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/              |           |\n",
      "|    mean_ep_length  | 200       |\n",
      "|    mean_reward     | -1.01e+03 |\n",
      "| time/              |           |\n",
      "|    total_timesteps | 2000      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 61.9      |\n",
      "|    critic_loss     | 0.0825    |\n",
      "|    ent_coef        | 0.216     |\n",
      "|    ent_coef_loss   | -0.336    |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 1899      |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.36e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 211       |\n",
      "|    time_elapsed    | 11        |\n",
      "|    total_timesteps | 2400      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 70.1      |\n",
      "|    critic_loss     | 0.369     |\n",
      "|    ent_coef        | 0.205     |\n",
      "|    ent_coef_loss   | 0.1       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 2299      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-241.03 +/- 98.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -241     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 75.7     |\n",
      "|    critic_loss     | 0.363    |\n",
      "|    ent_coef        | 0.213    |\n",
      "|    ent_coef_loss   | 0.283    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 200       |\n",
      "|    ep_rew_mean     | -1.13e+03 |\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 205       |\n",
      "|    time_elapsed    | 15        |\n",
      "|    total_timesteps | 3200      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 78        |\n",
      "|    critic_loss     | 0.483     |\n",
      "|    ent_coef        | 0.221     |\n",
      "|    ent_coef_loss   | 0.25      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 3099      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-149.66 +/- 42.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -150     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 81.3     |\n",
      "|    critic_loss     | 0.664    |\n",
      "|    ent_coef        | 0.216    |\n",
      "|    ent_coef_loss   | -0.266   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -945     |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -823     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 195      |\n",
      "|    time_elapsed    | 24       |\n",
      "|    total_timesteps | 4800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 85.8     |\n",
      "|    critic_loss     | 1.4      |\n",
      "|    ent_coef        | 0.165    |\n",
      "|    ent_coef_loss   | 0.0993   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4699     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-148.43 +/- 43.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -148     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 76.5     |\n",
      "|    critic_loss     | 0.898    |\n",
      "|    ent_coef        | 0.155    |\n",
      "|    ent_coef_loss   | -0.309   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -719     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 197      |\n",
      "|    time_elapsed    | 28       |\n",
      "|    total_timesteps | 5600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 78.3     |\n",
      "|    critic_loss     | 0.876    |\n",
      "|    ent_coef        | 0.137    |\n",
      "|    ent_coef_loss   | 0.0118   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5499     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-148.70 +/- 47.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -149     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 6000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 76.8     |\n",
      "|    critic_loss     | 1.42     |\n",
      "|    ent_coef        | 0.138    |\n",
      "|    ent_coef_loss   | -0.00535 |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -655     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 32       |\n",
      "|    total_timesteps | 6400     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 72.2     |\n",
      "|    critic_loss     | 1.11     |\n",
      "|    ent_coef        | 0.143    |\n",
      "|    ent_coef_loss   | -0.623   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6299     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-150.20 +/- 92.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -150     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 7000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 68.2     |\n",
      "|    critic_loss     | 1.53     |\n",
      "|    ent_coef        | 0.14     |\n",
      "|    ent_coef_loss   | -0.299   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -593     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 199      |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 7200     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 85.6     |\n",
      "|    critic_loss     | 1.89     |\n",
      "|    ent_coef        | 0.136    |\n",
      "|    ent_coef_loss   | 0.326    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7099     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-148.22 +/- 53.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -148     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 68.6     |\n",
      "|    critic_loss     | 1.36     |\n",
      "|    ent_coef        | 0.108    |\n",
      "|    ent_coef_loss   | 0.21     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -546     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 200      |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -515     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 200      |\n",
      "|    time_elapsed    | 43       |\n",
      "|    total_timesteps | 8800     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 66       |\n",
      "|    critic_loss     | 1.03     |\n",
      "|    ent_coef        | 0.096    |\n",
      "|    ent_coef_loss   | -0.227   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8699     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-99.17 +/- 48.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -99.2    |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 9000     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 75.8     |\n",
      "|    critic_loss     | 1.33     |\n",
      "|    ent_coef        | 0.0965   |\n",
      "|    ent_coef_loss   | 0.161    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8899     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -487     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 199      |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 9600     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 74.6     |\n",
      "|    critic_loss     | 1.92     |\n",
      "|    ent_coef        | 0.089    |\n",
      "|    ent_coef_loss   | -0.0916  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9499     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-142.73 +/- 45.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -143     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 61.7     |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.0819   |\n",
      "|    ent_coef_loss   | -0.0317  |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9899     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -457     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 199      |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 10400    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 60.5     |\n",
      "|    critic_loss     | 1.16     |\n",
      "|    ent_coef        | 0.0782   |\n",
      "|    ent_coef_loss   | -0.182   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10299    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-191.83 +/- 57.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -192     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 11000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 60.3     |\n",
      "|    critic_loss     | 1.38     |\n",
      "|    ent_coef        | 0.0816   |\n",
      "|    ent_coef_loss   | -0.67    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -436     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 11200    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 56.1     |\n",
      "|    critic_loss     | 2.18     |\n",
      "|    ent_coef        | 0.0815   |\n",
      "|    ent_coef_loss   | -0.499   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11099    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-149.60 +/- 93.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -150     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 12000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 58.6     |\n",
      "|    critic_loss     | 2.22     |\n",
      "|    ent_coef        | 0.0812   |\n",
      "|    ent_coef_loss   | -0.204   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11899    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -420     |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 198      |\n",
      "|    time_elapsed    | 60       |\n",
      "|    total_timesteps | 12000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 200      |\n",
      "|    ep_rew_mean     | -403     |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 199      |\n",
      "|    time_elapsed    | 64       |\n",
      "|    total_timesteps | 12800    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 53       |\n",
      "|    critic_loss     | 1.58     |\n",
      "|    ent_coef        | 0.0697   |\n",
      "|    ent_coef_loss   | -0.149   |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12699    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-179.06 +/- 72.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 200      |\n",
      "|    mean_reward     | -179     |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 13000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 59.5     |\n",
      "|    critic_loss     | 1.86     |\n",
      "|    ent_coef        | 0.0677   |\n",
      "|    ent_coef_loss   | 0.527    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12899    |\n",
      "---------------------------------\n",
      "Stopping training because there was no new best model in the last 4 evaluations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.sac.sac.SAC at 0x143fbf910>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "\n",
    "# Separate evaluation env\n",
    "eval_env = gym.make(\"Pendulum-v1\")\n",
    "# Stop training if there is no improvement after more than 3 evaluations\n",
    "stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\n",
    "eval_callback = EvalCallback(eval_env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "\n",
    "model = SAC(\"MlpPolicy\", \"Pendulum-v1\", learning_rate=1e-3, verbose=1)\n",
    "# Almost infinite number of timesteps, but the training will stop early\n",
    "# as soon as the the number of consecutive evaluations without model\n",
    "# improvement is greater than 3\n",
    "model.learn(int(1e10), callback=eval_callback)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
