{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gymnasium, SB3 실습\n",
    "\n",
    "목표:\n",
    "- Gymnasium 환경 생성 및 설정\n",
    "- SB3 agent 생성 및 학습\n",
    "- agent 성능 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 import\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO  # SB3 알고리즘 중 하나\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 환경 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 환경 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()  # 랜덤한 행동 선택\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # `done` 또는 `truncated` 중 하나라도 True이면 에피소드가 끝난 것\n",
    "    if done or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Agent로 환경 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1, Total Reward: 29.0\n",
      "Episode: 2, Total Reward: 18.0\n",
      "Episode: 3, Total Reward: 22.0\n",
      "Episode: 4, Total Reward: 16.0\n",
      "Episode: 5, Total Reward: 17.0\n"
     ]
    }
   ],
   "source": [
    "for episode in range(5):  # 5번의 에피소드 실행\n",
    "    observation, info = env.reset()  # info도 함께 받음\n",
    "    total_reward = 0\n",
    "\n",
    "    while True:\n",
    "        action = env.action_space.sample()  # 랜덤 행동\n",
    "        observation, reward, done, truncated, info = env.step(action)  # truncated 추가\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done or truncated:  # done 또는 truncated가 True이면 에피소드 종료\n",
    "            print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
    "            break\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 생성 및 학습\n",
    "\n",
    "- EvalCallback을 사용하여 학습 과정 log\n",
    "- tensorboard로 학습 과정 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=4000, episode_reward=149.40 +/- 27.41\n",
      "Episode length: 149.40 +/- 27.41\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 149      |\n",
      "|    mean_reward     | 149      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=152.40 +/- 25.88\n",
      "Episode length: 152.40 +/- 25.88\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 152      |\n",
      "|    mean_reward     | 152      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 23.2     |\n",
      "|    ep_rew_mean     | 23.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 1572     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=232.20 +/- 134.90\n",
      "Episode length: 232.20 +/- 134.90\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 232          |\n",
      "|    mean_reward          | 232          |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 12000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.013809332  |\n",
      "|    clip_fraction        | 0.209        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.682       |\n",
      "|    explained_variance   | -0.004468441 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.3          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0237      |\n",
      "|    value_loss           | 19.6         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=437.40 +/- 125.20\n",
      "Episode length: 437.40 +/- 125.20\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 437      |\n",
      "|    mean_reward     | 437      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 33.2     |\n",
      "|    ep_rew_mean     | 33.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 784      |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# 환경 생성\n",
    "env = make_vec_env(\"CartPole-v1\", n_envs=4)  # 병렬 환경을 생성\n",
    "\n",
    "# 모델 생성 (PPO 사용)\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# EvalCallback을 사용하여 학습 중에 모델을 평가\n",
    "eval_env = Monitor(gym.make(\"CartPole-v1\"))  # 평가 환경을 모니터링\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=1000,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "# 학습 시작\n",
    "model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "\n",
    "# 학습 완료 후 모델 저장\n",
    "model.save(\"ppo_cartpole\")\n",
    "\n",
    "# tensorboard로 학습 과정 시각화\n",
    "# 학습 중에 터미널에서 다음 명령어를 실행하여 TensorBoard를 확인하세요.\n",
    "# tensorboard --logdir ./logs/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 학습된 모델 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video ./videos/cartpole.mp4.\n",
      "Moviepy - Writing video ./videos/cartpole.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./videos/cartpole.mp4\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 학습된 모델 로드\n",
    "model = PPO.load(\"ppo_cartpole\")\n",
    "\n",
    "# 학습된 모델로 시각화\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "observation, info = env.reset()\n",
    "frames = []  # 프레임 저장 리스트\n",
    "\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(observation, deterministic=True)\n",
    "    observation, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    # 각 프레임을 저장 (이미지 시퀀스로 비디오 생성 가능)\n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "\n",
    "    if done or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "\n",
    "# MoviePy를 사용하여 비디오 저장\n",
    "import moviepy.editor as mpy\n",
    "\n",
    "video_dir = './videos'\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "clip = mpy.ImageSequenceClip(frames, fps=30)\n",
    "clip.write_videofile(f\"{video_dir}/cartpole.mp4\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 더 탐구해보기\n",
    "\n",
    "- 어떤 모델과 환경이 호환되지 않는 이유가 무엇일까요?\n",
    "- 학습된 모델의 성능을 높이기 위해 어떤 방법을 사용할 수 있을까요?\n",
    "- 여러 환경과 모델을 사용하여 학습을 진행하고, 성능을 비교해보세요.\n",
    "- 다양한 Wrappers, Callbacks를 활용하여 원하는 동작을 구현해보세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
