{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gymnasium, SB3 실습\n",
    "\n",
    "목표:\n",
    "- Gymnasium 환경 생성 및 설정\n",
    "- SB3 agent 생성 및 학습\n",
    "- agent 성능 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO  # SB3 알고리즘 중 하나인 PPO 사용\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 환경 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name, render_mode=\"rgb_array\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 환경 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(10):\n",
    "    env.render()\n",
    "    action = env.action_space.sample()  # 임의의 액션을 선택\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Random Agent로 환경 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 23.0\n"
     ]
    }
   ],
   "source": [
    "# Random agent가 CartPole 환경에서 어떻게 수행하는지 확인\n",
    "observation, info = env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "\n",
    "while True:\n",
    "    action = env.action_space.sample()  # 임의의 액션을 선택\n",
    "    obs, reward, terminated, truncated, info = env.step(action)  # 액션을 실행하고 결과 확인\n",
    "    total_reward += reward\n",
    "\n",
    "    if terminated or truncated:\n",
    "        print(f\"Total Reward: {total_reward}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 생성 및 학습\n",
    "\n",
    "- EvalCallback을 사용하여 학습 과정 log\n",
    "- tensorboard로 학습 과정 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Eval num_timesteps=4000, episode_reward=9.60 +/- 0.49\n",
      "Episode length: 9.60 +/- 0.49\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.6      |\n",
      "|    mean_reward     | 9.6      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=8000, episode_reward=9.20 +/- 0.75\n",
      "Episode length: 9.20 +/- 0.75\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9.2      |\n",
      "|    mean_reward     | 9.2      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.1     |\n",
      "|    ep_rew_mean     | 21.1     |\n",
      "| time/              |          |\n",
      "|    fps             | 3150     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=186.80 +/- 123.68\n",
      "Episode length: 186.80 +/- 123.68\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 187         |\n",
      "|    mean_reward          | 187         |\n",
      "| time/                   |             |\n",
      "|    total_timesteps      | 12000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015132526 |\n",
      "|    clip_fraction        | 0.239       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | 0.004204333 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.05        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0262     |\n",
      "|    value_loss           | 18.9        |\n",
      "-----------------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=16000, episode_reward=275.00 +/- 127.03\n",
      "Episode length: 275.00 +/- 127.03\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 275      |\n",
      "|    mean_reward     | 275      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 16000    |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 32.9     |\n",
      "|    ep_rew_mean     | 32.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 1367     |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "\n",
    "# PPO 모델 생성\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "# 학습 시 콜백 설정\n",
    "eval_env = Monitor(gym.make(\"CartPole-v1\"))\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=1000,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "# 학습\n",
    "model.learn(total_timesteps=10000, callback=eval_callback)\n",
    "\n",
    "# 학습 완료 후 모델 저장\n",
    "model.save(\"ppo_cartpole\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 학습된 모델 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "평균 보상: 348.5, 보상의 표준편차: 139.04837287793052\n",
      "Moviepy - Building video ./videos/cartpole.mp4.\n",
      "Moviepy - Writing video ./videos/cartpole.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready ./videos/cartpole.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# 학습된 모델 불러오기\n",
    "model = PPO.load(\"ppo_cartpole\")\n",
    "\n",
    "# 학습된 모델 평가\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, render=True)\n",
    "print(f\"평균 보상: {mean_reward}, 보상의 표준편차: {std_reward}\")\n",
    "\n",
    "# 평가 시각화\n",
    "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
    "observation, info = env.reset()\n",
    "frames = []\n",
    "\n",
    "for _ in range(1000):\n",
    "    action, _states = model.predict(observation, deterministic=True)\n",
    "    observation, rewards, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    frame = env.render()\n",
    "    frames.append(frame)\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset()\n",
    "\n",
    "env.close()\n",
    "\n",
    "import moviepy.editor as mpy\n",
    "\n",
    "video_dir = './videos'\n",
    "os.makedirs(video_dir, exist_ok=True)\n",
    "clip = mpy.ImageSequenceClip(frames, fps=30)\n",
    "clip.write_videofile(f\"{video_dir}/cartpole.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 더 탐구해보기\n",
    "\n",
    "- 어떤 모델과 환경이 호환되지 않는 이유가 무엇일까요?\n",
    "- 학습된 모델의 성능을 높이기 위해 어떤 방법을 사용할 수 있을까요?\n",
    "- 여러 환경과 모델을 사용하여 학습을 진행하고, 성능을 비교해보세요.\n",
    "- 다양한 Wrappers, Callbacks를 활용하여 원하는 동작을 구현해보세요."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
